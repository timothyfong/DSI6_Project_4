{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 4: Web Scraping Job Postings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "## Directions\n",
    "\n",
    "In this project you will be leveraging a variety of skills. The first will be to use the web-scraping and/or API techniques you've learned to collect data on data-related jobs from any job aggregator. Once you have collected and cleaned the data, use it to answer the two questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>\n",
    "\n",
    "**Focus on data-related job postings**, e.g. <font color=red>data scientist, data analyst, research scientist, business intelligence</font>, and any others you might think of. You may also want to decrease the scope by **limiting your search to a single region**.\n",
    "\n",
    "features to collect: <font color=red>location, title, summary of job</font>\n",
    "\n",
    "Main objectives:\n",
    "   1. Determine the <font color=red>industry factors</font> that are most important in <font color=red>predicting the salary amounts</font> for these data.\n",
    "   2. Determine the <font color=red>factors that distinguish job categories and titles</font> from each other. For example, can required skills accurately predict job title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from selenium import webdriver # initialize browser\n",
    "from time import sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Step 1 - extract job description links using 'element'</font>:\n",
    "``` Python\n",
    "# get all the links\n",
    "job_details = pd.DataFrame()\n",
    "jobs = []\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "for i in range(240):\n",
    "    url = \"https://www.website.com/search?search=data&sortBy=new_posting_date&page={}\".format(i)\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    try:\n",
    "        for link in soup.find_all('a', {'class':'bg-white mb3 w-100 dib v-top pa3 no-underline flex-ns flex-wrap JobCard__card___22xP3'}):\n",
    "            jobs.append('https://www.website.com' + link.get('href'))\n",
    "    except:\n",
    "        jobs.append('None')\n",
    "    \n",
    "    # check progress of url scraping\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "job_details['url'] = jobs\n",
    "job_details.to_csv('project4_urls.csv', index=False)\n",
    "\n",
    "job_details.head()\n",
    "driver.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Step 2 - extract job details & description from each url</font>\n",
    "\n",
    "``` Python\n",
    "job_details = pd.DataFrame(columns=['company_name','job_title','location','employment_type','seniority','job_category','salary_range','salary_freq','roles & responsibilities','requirements'])\n",
    "job_details['links'] = jobs\n",
    "                           \n",
    "for entry in html.find_all('div', {'class':'bg-white pa4'}):\n",
    "    name = entry.find('p', {'name':'company'}).text\n",
    "    job = entry.find('h1', {'id':'job_title'}).text\n",
    "    location = str(entry.find('a', {'href':'#location_map'}).renderContents())\n",
    "    emp_type = entry.find('p', {'id':'employment_type'}).text\n",
    "    seniority = entry.find('p', {'id':'seniority'}).text\n",
    "    job_categ = entry.find('p', {'id':'job-categories'}).text\n",
    "    salary_range = entry.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "    salary_freq = entry.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "    roles = entry.find('div', {'id':'description-content'}).text\n",
    "    requirements = entry.find('div', {'id':'requirements-content'}).text\n",
    "\n",
    "job_details.loc[len(job_details)] = [name, job, location, emp_type, seniority, job_categ, salary_range, salary_freq, roles, requirements]\n",
    "\n",
    "job_details\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Load csv with scraped urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('project4_urls.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "### split dataframe into sets of 500 urls each\n",
    "\n",
    "#### Scrape data for first set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[0:499].copy(deep=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[0:499]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "\n",
    "df1['company_name'] = company\n",
    "df1['job_title'] = position\n",
    "df1['location'] = address\n",
    "df1['employment_type'] = employment\n",
    "df1['seniority'] = snr\n",
    "df1['job_category'] = job_cat\n",
    "df1['salary_range'] = sal_r\n",
    "df1['salary_freq'] = sal_f\n",
    "df1['roles & responsibilities'] = r_and_r\n",
    "df1['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df1.head(10)\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('project4_df1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for second set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[500:999].copy(deep=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[500:999]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df2['company_name'] = company\n",
    "df2['job_title'] = position\n",
    "df2['location'] = address\n",
    "df2['employment_type'] = employment\n",
    "df2['seniority'] = snr\n",
    "df2['job_category'] = job_cat\n",
    "df2['salary_range'] = sal_r\n",
    "df2['salary_freq'] = sal_f\n",
    "df2['roles & responsibilities'] = r_and_r\n",
    "df2['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df2.head(10)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('project4_df2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for third set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.loc[1000:1499].copy(deep=True)\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[1000:1499]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df3['company_name'] = company\n",
    "df3['job_title'] = position\n",
    "df3['location'] = address\n",
    "df3['employment_type'] = employment\n",
    "df3['seniority'] = snr\n",
    "df3['job_category'] = job_cat\n",
    "df3['salary_range'] = sal_r\n",
    "df3['salary_freq'] = sal_f\n",
    "df3['roles & responsibilities'] = r_and_r\n",
    "df3['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df3.head(10)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('project4_df3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for fourth set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.loc[1500:1999].copy(deep=True)\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[1500:1999]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df4['company_name'] = company\n",
    "df4['job_title'] = position\n",
    "df4['location'] = address\n",
    "df4['employment_type'] = employment\n",
    "df4['seniority'] = snr\n",
    "df4['job_category'] = job_cat\n",
    "df4['salary_range'] = sal_r\n",
    "df4['salary_freq'] = sal_f\n",
    "df4['roles & responsibilities'] = r_and_r\n",
    "df4['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df4.head(10)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('project4_df4.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for fifth set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df.loc[2000:2499].copy(deep=True)\n",
    "df5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[2000:2499]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df5['company_name'] = company\n",
    "df5['job_title'] = position\n",
    "df5['location'] = address\n",
    "df5['employment_type'] = employment\n",
    "df5['seniority'] = snr\n",
    "df5['job_category'] = job_cat\n",
    "df5['salary_range'] = sal_r\n",
    "df5['salary_freq'] = sal_f\n",
    "df5['roles & responsibilities'] = r_and_r\n",
    "df5['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df5.head(10)\n",
    "print(df5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv('project4_df5.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for sixth set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df.loc[2500:2999].copy(deep=True)\n",
    "df6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[2500:2999]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df6['company_name'] = company\n",
    "df6['job_title'] = position\n",
    "df6['location'] = address\n",
    "df6['employment_type'] = employment\n",
    "df6['seniority'] = snr\n",
    "df6['job_category'] = job_cat\n",
    "df6['salary_range'] = sal_r\n",
    "df6['salary_freq'] = sal_f\n",
    "df6['roles & responsibilities'] = r_and_r\n",
    "df6['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df6.head(10)\n",
    "print(df6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.to_csv('project4_df6.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for seventh set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.loc[3000:3499].copy(deep=True)\n",
    "df7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[3000:3499]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df7['company_name'] = company\n",
    "df7['job_title'] = position\n",
    "df7['location'] = address\n",
    "df7['employment_type'] = employment\n",
    "df7['seniority'] = snr\n",
    "df7['job_category'] = job_cat\n",
    "df7['salary_range'] = sal_r\n",
    "df7['salary_freq'] = sal_f\n",
    "df7['roles & responsibilities'] = r_and_r\n",
    "df7['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df7.head(10)\n",
    "print(df7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7.to_csv('project4_df7.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for eighth set of 500 urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df.loc[3500:3999].copy(deep=True)\n",
    "df8.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[3500:3999]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df8['company_name'] = company\n",
    "df8['job_title'] = position\n",
    "df8['location'] = address\n",
    "df8['employment_type'] = employment\n",
    "df8['seniority'] = snr\n",
    "df8['job_category'] = job_cat\n",
    "df8['salary_range'] = sal_r\n",
    "df8['salary_freq'] = sal_f\n",
    "df8['roles & responsibilities'] = r_and_r\n",
    "df8['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df8.head(10)\n",
    "print(df8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.to_csv('project4_df8.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "#### Scrape data for remaining urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df.loc[4000:].copy(deep=True)\n",
    "df9.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company = []\n",
    "position = []\n",
    "address = []\n",
    "employment = []\n",
    "snr = []\n",
    "job_cat = []\n",
    "sal_r = []\n",
    "sal_f = []\n",
    "r_and_r = []\n",
    "req = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver/chromedriver\")\n",
    "\n",
    "for url in df['url'].loc[4000:]:\n",
    "    driver.get(url)\n",
    "    \n",
    "    sleep(3)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    x = response.status_code\n",
    "    if x != 200:\n",
    "        print('Status code:', response.status_code)\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    ##### get company name\n",
    "    try:\n",
    "        name = soup.find('p', {'name':'company'}).text\n",
    "        company.append(name)\n",
    "    except:\n",
    "        company.append('None')\n",
    "    ##### get job title\n",
    "    try:\n",
    "        job = soup.find('h1', {'id':'job_title'}).text\n",
    "        position.append(job)\n",
    "    except:\n",
    "        position.append('None')\n",
    "    ##### get company location\n",
    "    try:\n",
    "        location = soup.find('a', {'href':'#location_map'}).text\n",
    "        address.append(location)\n",
    "    except:\n",
    "        address.append('None')\n",
    "    ##### get employment type\n",
    "    try:\n",
    "        emp_type = soup.find('p', {'id':'employment_type'}).text\n",
    "        employment.append(emp_type)\n",
    "    except:\n",
    "        employment.append('None')\n",
    "    ##### get job seniority\n",
    "    try:\n",
    "        seniority = soup.find('p', {'id':'seniority'}).text\n",
    "        snr.append(seniority)\n",
    "    except:\n",
    "        snr.append('None')\n",
    "    ##### get job category\n",
    "    try:\n",
    "        job_categ = soup.find('p', {'id':'job-categories'}).text\n",
    "        job_cat.append(job_categ)\n",
    "    except:\n",
    "        job_cat.append('None')\n",
    "    ##### get salary range\n",
    "    try:\n",
    "        salary_range = soup.find('span', {'class':'salary_range dib f2-5 fw6 black-80'}).text\n",
    "        sal_r.append(salary_range)\n",
    "    except:\n",
    "        sal_r.append('None')\n",
    "    ##### get salary type\n",
    "    try:\n",
    "        salary_freq = soup.find('span', {'class':'salary_type dib f5 fw4 black-60 pr1 i pb'}).text\n",
    "        sal_f.append(salary_freq)\n",
    "    except:\n",
    "        sal_f.append('None')\n",
    "    ##### get job description\n",
    "    try:\n",
    "        roles = soup.find('div', {'id':'description-content'}).text\n",
    "        r_and_r.append(roles)\n",
    "    except:\n",
    "        r_and_r.append('None')\n",
    "    ##### get job requirements\n",
    "    try:\n",
    "        requirements = soup.find('div', {'id':'requirements-content'}).text\n",
    "        req.append(requirements)\n",
    "    except:\n",
    "        req.append('None')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i%50 == 0:\n",
    "        print(i)\n",
    "\n",
    "df9['company_name'] = company\n",
    "df9['job_title'] = position\n",
    "df9['location'] = address\n",
    "df9['employment_type'] = employment\n",
    "df9['seniority'] = snr\n",
    "df9['job_category'] = job_cat\n",
    "df9['salary_range'] = sal_r\n",
    "df9['salary_freq'] = sal_f\n",
    "df9['roles & responsibilities'] = r_and_r\n",
    "df9['requirements'] = req\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "df9.head(10)\n",
    "print(df9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.to_csv('project4_df9.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "\n",
    "### Combining dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "df1 = pd.read_csv('project4_df1.csv')\n",
    "print(df1.shape)\n",
    "\n",
    "df2 = pd.read_csv('project4_df2.csv')\n",
    "print(df2.shape)\n",
    "\n",
    "df3 = pd.read_csv('project4_df3.csv')\n",
    "print(df3.shape)\n",
    "\n",
    "df4 = pd.read_csv('project4_df4.csv')\n",
    "print(df4.shape)\n",
    "\n",
    "df5 = pd.read_csv('project4_df5.csv')\n",
    "print(df5.shape)\n",
    "\n",
    "df6 = pd.read_csv('project4_df6.csv')\n",
    "print(df6.shape)\n",
    "\n",
    "df7 = pd.read_csv('project4_df7.csv')\n",
    "print(df7.shape)\n",
    "\n",
    "df8 = pd.read_csv('project4_df8.csv')\n",
    "print(df8.shape)\n",
    "\n",
    "df9 = pd.read_csv('project4_df9.csv')\n",
    "print(df9.shape)\n",
    "\n",
    "frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "df = pd.concat(frames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "print(df.shape)\n",
    "df.to_csv('project4_compiled.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
